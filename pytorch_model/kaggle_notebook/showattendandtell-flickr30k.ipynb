{"cells":[{"metadata":{},"cell_type":"markdown","source":"Very minor differences in this model are \n1. The model dimensions are bigger. Batch size is 32.\n2. The threshold is 4.\n def deep_output_layer(self, embedded_caption, h, context_vector):\n        \"\"\"\n        :param embedded_caption: embedded caption, a tensor with shape (batch_size, embed_dim)\n        :param h: hidden state, a tensor with shape (batch_size, decoder_dim)\n        :param context_vector: context vector, a tensor with shape (batch_size, encoder_dim)\n        :return: output\n        \"\"\"\n        # Deep output is essentially multilayer perceptron for output\n        dropout = nn.Dropout(0.2)\n        scores = self.relu(dropout(self.L_h(h)))\n        scores = (self.fc(h))\n        return scores"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\ndata_location =  \"../input/flickr-image-dataset/flickr30k_images/\"\ncaption_file = '../input/captionstxt/captions.txt'\n","execution_count":75,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Custom dataset loader"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","execution_count":76,"outputs":[{"output_type":"execute_result","execution_count":76,"data":{"text/plain":"device(type='cuda')"},"metadata":{}}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import os\nimport pandas as pd\nimport spacy # for tokenizer\nimport torch\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import DataLoader, Dataset\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport torchvision.transforms as transforms\n\n# python -m spacy download en\nspacy_eng = spacy.load('en_core_web_sm')\n\nclass Vocab_Builder:\n    \n    def __init__ (self,freq_threshold):\n\n        # freq_threshold is to allow only words with a frequency higher \n        # than the threshold\n\n        self.itos = {0 : \"<PAD>\", 1 : \"<SOS>\", 2 : \"<EOS>\", 3 : \"<UNK>\"}  #index to string mapping\n        self.stoi = {\"<PAD>\" : 0, \"<SOS>\" : 1, \"<EOS>\" : 2, \"<UNK>\" : 3}  # string to index mapping\n        self.freq_threshold = freq_threshold\n\n    def __len__(self):\n        return len(self.itos)\n    \n    @staticmethod\n    def tokenizer_eng(text):\n        #Removing spaces, lower, general vocab related work\n\n        return [token.text.lower() for token in spacy_eng.tokenizer(text)]\n    \n    def build_vocabulary(self, sentence_list):\n        frequencies = {} # dict to lookup for words\n        idx = 4\n\n        # FIXME better ways to do this are there\n        for sentence in sentence_list:\n            for word in self.tokenizer_eng(sentence):\n                if word not in frequencies:\n                    frequencies[word] = 1\n                else:\n                    frequencies[word] += 1 \n                if(frequencies[word] == self.freq_threshold):\n                    #Include it\n                    self.stoi[word] = idx\n                    self.itos[idx] = word\n                    idx += 1\n    \n    # Convert text to numericalized values\n    def numericalize(self,text):\n        tokenized_text = self.tokenizer_eng(text) # Get the tokenized text\n        \n        # Stoi contains words which passed the freq threshold. Otherwise, get the <UNK> token\n        return [self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"]\n        for token in tokenized_text ]\n    \n    def denumericalize(self, tensors):\n        text = [self.itos[token] if token in self.itos else self.itos[3]]\n        return text\n\nclass FlickrDataset(Dataset):\n    def __init__(self, root_dir, captions_file, test, transform = None, freq_threshold = 5):\n        self.root_dir = root_dir\n        self.df = pd.read_csv(caption_file , delimiter='|')\n        self.transform = transform\n        # Get images, caption column from pandas\n        self.split_factor = 153915 # 4000/ 5 = reserving 200 images for testing\n        \n        self.imgs = self.df[\"image_name\"]\n        self.imgs_test = self.imgs[self.split_factor:]\n        self.imgs = self.imgs[0:self.split_factor]\n        self.captions = self.df[\"caption_text\"]\n        self.captions_test = self.captions[self.split_factor:]\n        self.captions = self.captions[0:self.split_factor]\n        self.test = test\n        #Init and Build vocab\n        self.vocab = Vocab_Builder(freq_threshold) # freq threshold is experimental\n        self.vocab.build_vocabulary(self.captions.tolist())\n\n    def __len__(self):\n        if (self.test == True):\n            return len(self.imgs_test)\n        \n        return len(self.imgs)\n    \n    def __getitem__(self, index: int):\n        \n        if self.test == False:\n            caption = self.captions[index]\n            img_id = self.imgs[index]\n        elif self.test == True:\n            index += self.split_factor\n            caption = self.captions_test[index]\n            img_id = self.imgs_test[index]\n            \n        img = Image.open(os.path.join(self.root_dir, img_id)).convert(\"RGB\")\n        if self.transform is not None:\n            img = self.transform(img)\n        \n        numericalized_caption = [self.vocab.stoi[\"<SOS>\"]] #stoi is string to index, start of sentence\n        numericalized_caption += self.vocab.numericalize(caption) # Convert each word to a number in our vocab\n        numericalized_caption.append(self.vocab.stoi[\"<EOS>\"])\n\n        #return tensor\n        \n        return img, torch.tensor(numericalized_caption)\n    \n    @staticmethod\n    def evaluation(self, index : int):\n        caption = self.captions_test[index]\n        img_id = self.imgs_test[index]\n        img = Image.open(os.path.join(self.root_dir, img_id)).convert(\"RGB\")\n\n        if self.transform is not None:\n            img = self.transform(img)\n        caption = self.vocab.tokenizer_eng(caption)\n        return img, caption\n# Caption lengths will be different, in our batch all have to be same length\n\n\n'''\nGoes to the dataloader\n'''\nclass Collate:\n    def __init__(self, pad_idx):\n        self.pad_idx = pad_idx\n\n    def __call__(self, batch):\n        imgs = [item[0].unsqueeze(0) for item in batch]\n        \n        imgs = torch.cat(imgs, dim=0)\n        \n        targets = [item[1] for item in batch]\n        \n        lengths = torch.tensor([len(cap) for cap in targets]).long()\n        \n        targets = pad_sequence(targets, batch_first=True, padding_value=self.pad_idx)\n        \n        return imgs, targets, lengths\n\n# caption file, Maybe change num_workers\n\ndef get_loader( root_folder,annotation_file,  transform, batch_size = 32,  num_workers = 8, shuffle = True, pin_memory = False, test = False):\n    \n\n\n    dataset =  FlickrDataset(root_folder,  annotation_file, test, transform = transform)\n    pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n\n    loader = DataLoader(\n        dataset = dataset,\n        batch_size = batch_size,\n        num_workers = num_workers,\n        shuffle = shuffle,\n        pin_memory = pin_memory,\n        collate_fn =  Collate(pad_idx = pad_idx)\n    )\n\n    return loader, dataset","execution_count":77,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# vocabulary = Vocab_Builder(freq_threshold = 4)\n\n# df.columns = ['image_name', 'comment_number', 'comment']\n# captions = df['comment']\n# vocabulary.build_vocabulary(captions.tolist())\n# print(len(vocabulary))","execution_count":78,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import pickle\n# with open('vocab.pickle', 'wb') as f:\n#     pickle.dump(vocabulary, f, protocol=pickle.HIGHEST_PROTOCOL)","execution_count":79,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.models as models\n# resnet50 = models.resnet50(pretrained = True)\nimport torch.nn.functional as F\n\n","execution_count":80,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class BahdanauAttention(nn.Module):\n    \n    '''\n    Soft attention which is deterministic in nature. First introducted in \n    the paper Neural Machine Translation by Jointly Learning to Align and Translate (Bahdanau Et Al)\n    '''\n    \n    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n        \n        # Get the L attention dimension vector using this. Pass through softmax to get the \n        # score\n        super(BahdanauAttention, self).__init__()\n        \n        self.attention = nn.Linear(attention_dim, 1)\n\n        self.softmax = nn.Softmax(dim=1)\n        \n        self.relu = nn.ReLU()\n        \n        self.encoder_to_attention_dim = nn.Linear(encoder_dim, attention_dim)\n        \n        self.decoder_to_attention_dim = nn.Linear(decoder_dim, attention_dim)\n        \n        self.dropout = nn.Dropout(0.5)\n        \n        self.tanh = nn.Tanh()\n        \n    def forward(self, encoder_output, hidden_states):\n        \n        '''\n        encoder_output : shape (batch_size, L, D)\n        decoder_output : shape (batch_size, hidden_state dimension) \n        \n        '''\n      \n        \n        encoder_attention = self.encoder_to_attention_dim(encoder_output) # (batch_size, L, attention_dim)\n        \n        decoder_attention = self.decoder_to_attention_dim(hidden_states) # (batch_size, attention_dim)\n        \n        # Torch.cat() ?? \n        # >>> a = torch.cat((encoder,decoder.unsqueeze(1)),dim=1)\n        # No, its actually adds the dim = 1 (Adds one more item in dim = 1)\n        # We just want to add.\n        \n        \n        #   (batch_size, L, attention_dim) + (batch_size, 1, attention_dim) \n        encoder_decoder = encoder_attention + decoder_attention.unsqueeze(1)  # (batch_size, L, attention_dim)\n        \n        encoder_decoder = self.tanh(encoder_decoder)\n        \n        attention_full = (self.attention(encoder_decoder)).squeeze(2) # (batch_size, L)\n        \n        alpha = self.softmax(attention_full) # Take the softmax across L(acc to paper)\n        \n        \n        '''\n        Equation 13 in the paper - classic Bahdanau attention\n        '''\n        \n        z = (encoder_output * alpha.unsqueeze(2) ).sum(dim = 1) # Sum across L (pixels)\n        \n        return z, alpha\n        \n        \n","execution_count":81,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n# Major changes include the ignoring of the last two layers. Author use a lower layer for more dense features.\n\n\nclass EncoderCNN(nn.Module):\n   \n    '''Takes in the image, encode it in shape (L,D) and return to decoder\n        \n        \"The extractor produces L vectors, each of which is\n        a D-dimensional representation corresponding to a part of\n        the image\"\n        \n    '''\n\n    def __init__(self, encoded_size=14, train_CNN = False):\n        \n        super(EncoderCNN, self).__init__()\n        \n        # Fine-tune parameter\n        self.train_CNN = train_CNN\n        \n        self.encoded_size =encoded_size\n        \n        # Load the resnet\n#         self.resnet50 = models.resnet50(pretrained=True)\n        self.resnet50 = models.resnet50(pretrained = False)\n        \n        # Remove adaptive pool and FC from the end\n        layers_to_use = list(self.resnet50.children())[:-3]\n        \n        # Unpack and make it the conv_net\n        self.resnet = nn.Sequential(*layers_to_use)\n        \n#         self.fc = nn.Linear(in_features,encoded_size)\n        \n        self.adaptive_pool = nn.AdaptiveAvgPool2d((encoded_size, encoded_size))\n        \n        self.relu = nn.ReLU()\n        \n        self.dropout = nn.Dropout(0.5)\n        \n        if not train_CNN:\n            for param in self.resnet.parameters():\n                param.requires_grad = False\n           \n        \n    def forward(self, images):\n         \n        # images.shape (batch_size, 3, image_size, image_size)    \n            \n        # Change the image_size dimensions. Check them yourself.\n        batch_size = images.shape[0]\n        \n        with torch.no_grad():\n            features = self.resnet(images) # Shape : (batch_size, encoder_dim, image_size/32, image_size/32)\n        \n        features = self.adaptive_pool(features) # Shape (batch_size, encoder_dim, encoded_size, encoded_size)\n        \n        features = features.permute(0, 2, 3, 1) # Shape : (batch_size, encoded_size, encoded_size, encoder_dim)\n        \n        # The above transformation is needed because we are going to do some computation in the \n        # decoder.\n        encoder_dim = features.shape[-1]\n        # When in doubt https://stackoverflow.com/questions/42479902/how-does-the-view-method-work-in-pytorch\n        features = features.view(batch_size, -1, encoder_dim)  # (batch_size, L, D)\n        \n#         print(\"-\" * 80 )\n        \n#         print(\"Features shape : \" , features.shape)\n        \n#         print(\"-\" * 80 )\n        \n        return features\n    \n    \n# In decoder, we use an LSTM cell. So, remove num_layers\n# https://stackoverflow.com/questions/57048120/pytorch-lstm-vs-lstmcell\n# In seq to seq model, it's more like gettign the state and ending the for loop when \n# you get the <EOS>\n\nclass Decoder(nn.Module):\n    def __init__(self,encoder_dim, decoder_dim, embed_size, vocab_size, attention_dim, dropout = 0.5):\n        \n        super(Decoder,self).__init__()\n        \n        # Setting everything for the perfect model!\n        \n        self.embed = nn.Embedding(vocab_size, embed_size) # Embedding layer courtesy Pytorch\n        \n        \n        self.encoder_dim = encoder_dim\n        \n        self.decoder_dim = decoder_dim\n        \n        self.attention_dim = attention_dim\n        \n        self.embed_dim = embed_size\n        \n        self.vocab_size = vocab_size\n        \n        self.dropout = nn.Dropout(0.2)\n        \n        # Note, it's an LSTM Cell, features + embedding\n        self.lstm = nn.LSTMCell(self.encoder_dim + self.embed_dim, self.decoder_dim, bias=True)\n        \n        self.attention = BahdanauAttention(encoder_dim, decoder_dim, attention_dim)\n        \n        self.f_beta = nn.Linear(self.decoder_dim, self.encoder_dim)\n        \n        self.sigmoid = nn.Sigmoid()\n        \n        self.relu = nn.ReLU()\n        \n        # See the paper \n        '''\n        The initial memory state and hidden state of the LSTM\n        are predicted by an average of the annotation vectors fed.\n        through two separate MLPs (init,c and init,h):\n        '''\n        \n        self.init_h = nn.Linear(encoder_dim, decoder_dim)\n        self.init_c = nn.Linear(encoder_dim, decoder_dim)\n        \n        # deep output layers\n        self.L_h = nn.Linear(decoder_dim, embed_size)\n        # self.L_z = nn.Linear(encoder_dim, embed_size)\n        # self.L_o = nn.Linear(embed_size, vocab_size)\n        \n        self.tanh = nn.Tanh()\n        self.fc = nn.Linear(decoder_dim, vocab_size)  # linear layer to find scores over vocabulary\n        self.init_weights()\n        self.init_weights()\n        \n    # Encoder output is the annotated vector a (L,D) in the paper\n    \n    def initialise_hidden_states(self, encoder_output):\n        \n        '''\n        Initialise the hidden states before forward prop. As given in the paper.\n        Authors take the mean of annotation vector across L dimension. Pass it through an MLP.\n        '''\n        \n        # encoder_output : shape (batch_size, L, encoder_dim=D)\n        \n        mean = (encoder_output).mean(dim = 1) # Take mean over L\n        \n        # Pass through Fully connected\n        \n        c_0 = self.init_c(mean)\n        c_0 = self.tanh(c_0)\n        h_0 = self.init_h(mean)\n        h_0 = self.tanh(h_0)\n        \n        return h_0, c_0, \n\n    def init_weights(self):\n        \n        # This helps initially. Fill the following weights before starting.\n        \n        self.embed.weight.data.uniform_(-0.1,0.1)\n        self.fc.weight.data.uniform_(-0.1,0.1)\n        self.fc.bias.data.fill_(0)\n    \n    \n    # Thankful to sgrvinod for this part. b\n    def forward(self, encoder_output, caption, caption_lengths):\n        \n        '''\n        encoder_output : shape(batch_size, L, D)\n        caption : (max_length, batch_size )\n        \n        Get the encoder_output i.e the features.\n        '''\n        \n        batch_size = encoder_output.size(0)\n        # num_pixels \n        L = encoder_output.size(1)\n        \n        max_caption_length = caption.shape[-1] # shape : (batch_size, max_caption) \n        \n        caption_lengths, sort_ind = caption_lengths.sort(dim=0, descending=True)\n        encoder_output = encoder_output[sort_ind]\n        caption = caption[sort_ind]\n        \n#         print(sort_ind)\n        \n        lengths = [l - 1 for l in caption_lengths]\n        \n        embedding_of_all_captions = self.embed(caption)\n        \n        predictions = torch.zeros(batch_size, max_caption_length - 1, self.vocab_size).to(device)\n        alphas = torch.zeros(batch_size, max_caption_length - 1, L).to(device)  \n        \n        # Concat and pass through lstm to get hidden states\n        \n        h, c = self.initialise_hidden_states(encoder_output)\n        \n        \n        \n        # Exclude <EOS>, t is the th timestep\n        # We get all the embeddings for the t timestep\n        # Then we get the encoded_output aka annotation vector\n        # Use soft attention to get the context vector.\n        # Concat and pass through the lstm cell to get hidden states --> predictions\n        \n#         print(max_caption_length)\n        \n        for t in range(max_caption_length - 1):\n            \n            batch_size_t = sum([l > t for l in lengths]) \n            \n            # z from the returning function\n            context_vector, alpha = self.attention(encoder_output[:batch_size_t], h[:batch_size_t])\n            \n            # Changes inspirsed from SgdrVinod(Suggested in paper also)\n            gate = self.sigmoid(self.f_beta(h[:batch_size_t]))\n            \n            gated_context = gate * context_vector\n#             context_vector : torch.Size([32, 1024]), embedded_caption_t : torch.Size([32, 256])\n\n            h, c = self.lstm(torch.cat([ embedding_of_all_captions[:batch_size_t,t,:], gated_context], dim=1),(h[:batch_size_t], c[:batch_size_t]))\n            \n            predict_deep = self.deep_output_layer(embedding_of_all_captions[:batch_size_t,t,:], h, context_vector)\n            \n            predictions[:batch_size_t, t, :] = predict_deep \n            \n            alphas[:batch_size_t, t, :] = alpha\n            \n        return predictions, alphas, caption, lengths\n        \n        \n \n    def deep_output_layer(self, embedded_caption, h, context_vector):\n        \"\"\"\n        :param embedded_caption: embedded caption, a tensor with shape (batch_size, embed_dim)\n        :param h: hidden state, a tensor with shape (batch_size, decoder_dim)\n        :param context_vector: context vector, a tensor with shape (batch_size, encoder_dim)\n        :return: output\n        \"\"\"\n        # Deep output is essentially multilayer perceptron for output\n        scores = self.relu(self.dropout(self.L_h(h)))\n        scores = (self.fc(h))\n        return scores\n    \n    def predict_caption(self, encoder_output, captions):\n        \n        # \"<SOS>\" 1\n        caption_list = [1]\n        alphas = [] \n        h, c = self.initialise_hidden_states(encoder_output)\n        \n        \n        # 2 is <EOS>\n        while len(caption_list) < 40 :\n            word = caption_list[-1]\n            \n            embedded_caption = self.embed(  torch.LongTensor([word]).to(device)  )  # (1, embed_dim)\n            \n            context_vector, alpha = self.attention(encoder_output, h);\n            \n            gate = self.sigmoid(self.f_beta(h))\n            \n            gated_context = gate * context_vector\n            \n            h, c = self.lstm(torch.cat([embedded_caption, gated_context], dim=1), (h, c))\n            \n            predictions = self.deep_output_layer(embedded_caption, h, context_vector)  # (1, vocab_size)\n            \n            # item converts to python scalar otherwise expect CUDA re-assert trigger\n            \n            next_word = (torch.argmax(predictions, dim=1, keepdim=True).squeeze()).item()\n            \n            caption_list.append(next_word)\n            \n            alphas.append(alpha)\n            \n            if(caption_list[-1] == 2):\n                break\n        return caption_list, alphas\n      \n    def beam_search(self, encoder_output, beam_size = 3):\n        \n        k = beam_size\n        \n        vocab_size = self.vocab_size\n        \n        encoder_size = encoder_output.size(-1)\n        \n        encoder_output = encoder_output.view(1, -1, encoder_size)\n        \n        num_pixels = encoder_output.size(1)\n        \n        encoder_output = encoder_output.expand(k, num_pixels, encoder_size)  # (k, num_pixels, encoder_dim)\n        \n        # Vocab.stoi(SOS)\n        k_prev_words = torch.LongTensor([[1]] * k).to(device) \n        seqs = k_prev_words\n        \n        top_k_scores = torch.zeros(k, 1).to(device)  # (k, 1)\n        \n        complete_seqs = list()\n        complete_seqs_scores = list()\n        \n        step = 1\n        \n        h, c = self.initialise_hidden_states(encoder_output)\n        \n        while True:\n            embedded_caption = self.embed(k_prev_words).squeeze(1)\n            \n            context_vector, alpha = self.attention(encoder_output, h);\n            \n            gate = self.sigmoid(self.f_beta(h))\n            \n            gated_context = gate * context_vector\n            \n            h, c = self.lstm(torch.cat([embedded_caption, gated_context], dim=1), (h, c))\n            \n            scores = self.deep_output_layer(embedded_caption, h, context_vector)\n            \n            scores = F.log_softmax(scores, dim=1)\n            \n            scores = top_k_scores.expand_as(scores) + scores\n            \n            if step == 1:\n                top_k_scores, top_k_words = scores[0].topk(k, dim=0)  # (s)\n            else:\n                top_k_scores, top_k_words = scores.view(-1).topk(k, dim=0)  # (s)\n                \n            prev_word_inds = torch.true_divide(top_k_words , vocab_size).long().cpu()  # (s)\n            next_word_inds = top_k_words % vocab_size  # (s)\n            \n             # Add new words to sequences\n            seqs = torch.cat([seqs[prev_word_inds], next_word_inds.unsqueeze(1)], dim=1)  # (s, step+1)\n\n            # Which sequences are incomplete (didn't reach <EOS>)?\n            incomplete_inds = [ind for ind, next_word in enumerate(next_word_inds) if\n                               next_word != 2 ] #vocab.itos['<EOS>']\n            complete_inds = list(set(range(len(next_word_inds))) - set(incomplete_inds))\n\n            # Set aside complete sequences\n            if len(complete_inds) > 0:\n                complete_seqs.extend(seqs[complete_inds].tolist())\n                complete_seqs_scores.extend(top_k_scores[complete_inds])\n            \n            k -= len(complete_inds)  # reduce beam length accordingly\n\n            # Proceed with incomplete sequences\n            if k == 0:\n                break\n                \n            seqs = seqs[incomplete_inds]\n            \n            h = h[prev_word_inds[incomplete_inds]]\n            c = c[prev_word_inds[incomplete_inds]]\n            \n            encoder_output = encoder_output[prev_word_inds[incomplete_inds]]\n            top_k_scores = top_k_scores[incomplete_inds].unsqueeze(1)\n            k_prev_words = next_word_inds[incomplete_inds].unsqueeze(1)\n\n            if step > 50:\n                break\n            step += 1\n        \n        if len(complete_seqs_scores) > 0:\n            i = complete_seqs_scores.index(max(complete_seqs_scores))\n            seq = complete_seqs[i]\n            return seq\n        else:\n            return [1,2]\n        return complete_seqs\n\n\n         ","execution_count":82,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torchvision.transforms as transforms\nfrom PIL import Image\n\n\ndef save_checkpoint(state, filename=\"./LastModel.pth.tar\"):\n    print(\"=> Saving checkpoint\")\n    torch.save(state, filename)\n\n\ndef load_checkpoint(checkpoint, model, optimizer):\n    print(\"=> Loading checkpoint\")\n    model.load_state_dict(checkpoint[\"state_dict\"])\n    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n    step = checkpoint[\"step\"]\n    return step","execution_count":83,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib import pyplot as plt\nimport random\n\n\ndef show_image(inp, title=None):\n    \"\"\"Imshow for Tensor.\"\"\"\n    inp = inp.numpy().transpose((1, 2, 0))\n    plt.imshow(inp)\n    \n \n    if title is not None:\n        plt.title(title)\n    plt.savefig('foo.png', bbox_inches='tight')\n    \n    plt.pause(0.001)  # pause a bit so that plots are updated","execution_count":84,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nfrom torch.utils.tensorboard import SummaryWriter\nimport torch.nn.functional as F\nfrom torch.nn.utils.rnn import pack_padded_sequence\n# from convert_text import get_loader\n# from utils import *\n# from model import CNNtoRNN\n# from utils save_checkpoint, load_checkpoint, print_examples\n","execution_count":85,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training loop\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nmean = [0.485, 0.456, 0.406]\n\nstd = [0.229, 0.224, 0.225]\n\ntransform = transforms.Compose(\n    [transforms.Resize((256,256)),\n    transforms.ToTensor(),\n    transforms.RandomHorizontalFlip(),\n    transforms.Normalize(mean, std)]\n)\ntransform_val = transforms.Compose(\n    [transforms.Resize((256,256)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean, std)]\n)\n\ntrain_loader, dataset = get_loader(\n    root_folder = data_location+\"/flickr30k_images\",\n    annotation_file = '../input/captionstxt/captions.txt',\n    transform = transform, \n    num_workers = 4,\n    test = False\n)\ntest_loader, test_dataset = get_loader(\n    root_folder = data_location+\"/flickr30k_images\",\n    annotation_file = '../input/captionstxt/captions.txt',\n    transform = transform_val, \n    num_workers = 4,\n    test = True\n)\n\n\n# Test_dataset gonna come here soon\n# Think about that later. We will do some training phases. It will take time but keep your calm.\n\ntorch.backends.cudnn.benchmark = True # Get some boost probaby\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nload_model = True\n\nsave_model = True\n\ntrain_CNN = False\n#Hyperparameters\n\nembed_size = 350\nencoder_dim = 1024\ndecoder_dim = 512\nattention_dim = 512\nvocab_size = len(dataset.vocab)\nlearning_rate = 4e-4 # Earlier 4e-4 till epoch 7 and then 2e-4\nnum_epochs = 15\n\nprint(\"Vocab_size\", vocab_size)\n\n# Tensorboard\nwriter = SummaryWriter(\"runs/flickr\")\n\nstep = 0\n# init model, loss\nencoder = EncoderCNN() # Default arguments already given as encoder_size 14, train_CNN = False\nencoder = encoder.to(device)\n\ndecoder = Decoder(encoder_dim, decoder_dim, embed_size, vocab_size, attention_dim)    \ndecoder = decoder.to(device)\n\nprint(device)\n\nalpha_c = 1.0  # Not used variable in code, used just 1.0  \n# regularization parameter for 'doubly stochastic attention', as in the paper\n\n# https://pytorch.org/tutorials/beginner/torchtext_translation_tutorial.html\n# criterion = nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<PAD>\"]).to(device)\n\ncriterion = nn.CrossEntropyLoss().to(device)\n\noptimizer = optim.Adam(decoder.parameters(), lr = learning_rate)\n\nencoder.train()\ndecoder.train()\n\nif load_model:\n    # load the decoder weights\n    step = load_checkpoint(torch.load(\"./Flickr30k_Decoder_11.pth.tar\"), decoder, optimizer)\n    \n    # load the encoder weights\n    m_state_dict = torch.load('./resnet5011.pt')\n    encoder.load_state_dict(m_state_dict)\n    encoder = encoder.to(device)\n    \n    print(encoder.training, decoder.training)\n    \n # Do after epoch 10 .. 14\nfor epoch in range(11,13):\n    if save_model:\n        checkpoint = {\n            \"state_dict\" : decoder.state_dict(),\n            \"optimizer\" : optimizer.state_dict(),\n            \"step\" : step\n        }\n        # This also got saved when epoch 6 started(latest)\n        # torch.save(encoder.state_dict(), 'resnet50.pt')\n        for param_group in optimizer.param_groups:\n            print(param_group['lr'])\n            param_group['lr'] = 4e-5\n            print(\"After\", param_group['lr'])\n        \n        \n        if epoch > 11:\n            filename = './Flickr30k_Decoder_' + str(epoch) +  '.pth.tar'\n            print(\"Saving in \", filename, epoch)\n            save_checkpoint(checkpoint, filename)\n            torch.save(encoder.state_dict(), 'resnet50' + str(epoch) + '.pt')\n    losses = []\n    mvl = []\n    for idx, (imgs, captions, lengths) in enumerate(train_loader):\n#         optimizer.zero_grad() Init config\n\n        imgs = imgs.to(device)\n        captions = captions.to(device)\n        lengths = lengths.to(device)\n        # Pass through the encoder and get the annotation vector\n        encoded_images = encoder(imgs)\n\n        scores, alphas, sorted_cap, decode_lengths = decoder(encoded_images, captions, lengths)\n        \n        # We don't want <SOS>\n        sorted_cap = sorted_cap[:,1:] # shape (batch_size, max_caption)\n        \n        # Remove timesteps that we didn't decode at, or are pads\n        # pack_padded_sequence is an easy trick to do this\n        \n        scores = pack_padded_sequence(scores, decode_lengths, batch_first=True).data\n        targets = pack_padded_sequence(sorted_cap, decode_lengths, batch_first=True).data\n        \n\n        # Calculate loss\n        loss = criterion(scores, targets)\n        \n        # batch_size = sorted_cap.shape[0]\n        # caption_length = sorted_cap.shape[1]\n        # loss = criterion(predictions.view(batch_size * caption_length, -1), sorted_cap.reshape(-1))\n        # Doubly stochastic attention regularization but not using the beta so not proper\n        loss += 1.0 * ((1. - alphas.sum(dim=1)) ** 2).mean()\n\n        losses.append(loss.item())\n        \n        decoder.zero_grad()\n        encoder.zero_grad()\n        \n        loss.backward()\n        \n        if( idx % 200 == 0):\n            print(\"Step\", idx, loss.item())\n\n        writer.add_scalar(\"training loss\", loss.item(), global_step = step)\n\n        step += 1\n\n        optimizer.step()\n\n        if (idx)% 2000 == 0:\n            print(\"Epoch: {} loss: {:.5f}\".format(epoch,loss.item()))\n            encoder.eval()\n            decoder.eval()\n            with torch.no_grad():\n                bleu_score_checker() # BLEU should be done separately\n                dataiter = iter(train_loader)\n                imgs,captions, lengths = next(dataiter)\n                imgs = imgs\n                captions = captions.to(device)\n                encoded_output = encoder( (imgs[0].unsqueeze(0).to(device)) )\n\n                # Does not make a difference for the caption since we are not using it\n                caption_greedy, alphas = decoder.predict_caption(encoded_output, captions)\n                caps_greedy =[dataset.vocab.itos[idx] for idx in caption_greedy]\n                caption = ' '.join(caps_greedy)\n                print(\"Greedy search\", caption)\n                \n                caption = decoder.beam_search(encoded_output, 3)\n                caps = [dataset.vocab.itos[idx] for idx in caption]\n                print(\"Beam search\", ' '.join(caps) )\n\n                show_image(imgs[0],title=' '.join(caps))\n            decoder.train()\n            encoder.train()\n            \n        # Valid loss\n        if (idx ) % 1000 == 0 :\n            valid_losses = []\n            decoder.eval()\n            encoder.eval()\n            print(\"Valid section\")\n            with torch.no_grad():\n                for index, (imgs, captions, lengths) in enumerate(test_loader):\n                    imgs = imgs.to(device)\n                    captions = captions.to(device)\n                    lengths = lengths.to(device)\n                    \n                    encoded_images = encoder(imgs)\n                    scores, alphas, sorted_cap, decode_lengths = decoder(encoded_images, captions, lengths)\n\n                    # We don't want <SOS>\n                    sorted_cap = sorted_cap[:,1:] # shape (batch_size, max_caption)\n\n                    scores = pack_padded_sequence(scores, decode_lengths, batch_first=True).data\n                    targets = pack_padded_sequence(sorted_cap, decode_lengths, batch_first=True).data\n\n                    valid_loss = criterion(scores, targets)\n                    # batch_size = sorted_cap.shape[0]\n                    # caption_length = sorted_cap.shape[1]\n                    # valid_loss = criterion(predictions.view(batch_size * caption_length, -1), sorted_cap.reshape(-1))\n                    \n                    valid_loss += 1.0 * ((1. - alphas.sum(dim=1)) ** 2).mean()\n                    \n                    valid_losses.append(valid_loss.item())\n\n                    # print(\"Step\", index, valid_loss.item())\n            decoder.train()\n            encoder.train()\n            print(\"-\" * 80)\n            mean_valid_loss = sum(valid_losses)/len(valid_losses)\n            mvl.append(mean_valid_loss)\n            print(mean_valid_loss)        \n            print(\"-\" * 80)                        \n    \n    mean_loss = sum(losses)/len(losses)\n    print(\"-\" * 80)\n    print(\"Mean loss\", mean_loss)\n    print(mvl)\n    print(\"-\" * 80)\n\n\n","execution_count":87,"outputs":[{"output_type":"stream","text":"Vocab_size 7547\ncuda\n=> Loading checkpoint\nTrue True\n4e-05\nAfter 4e-05\nStep 0 2.7402756214141846\nEpoch: 10 loss: 2.74028\n","name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-87-c557fde163a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m                 \u001b[0mbleu_score_checker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# BLEU should be done separately\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m                 \u001b[0mdataiter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcaptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataiter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-48-ae5b31cd95ce>\u001b[0m in \u001b[0;36mbleu_score_checker\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mtemp_gc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaption\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaption\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m             \u001b[0mtemp_gc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaption\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_gc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-77-83d1e3efa130>\u001b[0m in \u001b[0;36mevaluation\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mcaption\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptions_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mimg_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    900\u001b[0m         \"\"\"\n\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 902\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"P\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m                             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m                             \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{},"cell_type":"markdown","source":"Epoch 10 --> 11 best till now.\nBLEU-1 0.5930380578473908\nBLEU-2 0.40598468860863113\nBLEU-3 0.2727534747496662\nBLEU-4 0.1858028279503759\nNltk metrics\nBLEU-1 0.6111496417318056\nBLEU-2 0.4183835766419379\nBLEU-3 0.28463034547490357\nBLEU-4 0.19147730644802838"},{"metadata":{},"cell_type":"markdown","source":"Torch metrics epoch 10 --> epoch 11\nBLEU-1 0.5920705441713625\nBLEU-2 0.4084211654090082\nBLEU-3 0.28045407870035294\nBLEU-4 0.19565882267053974"},{"metadata":{},"cell_type":"markdown","source":"In between epoch 9\nTorch metrics\nBLEU-1 0.5865305591713187\nBLEU-2 0.4036900261383535\nBLEU-3 0.2721799185648416\nBLEU-4 0.186270821154543"},{"metadata":{},"cell_type":"markdown","source":"## Saving resnet weights for deployment purpose"},{"metadata":{"trusted":true},"cell_type":"code","source":"from torchtext.data.metrics import bleu_score","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef show_image2(inp, index, title=None):\n    \"\"\"Imshow for Tensor.\"\"\"\n    inp = inp.numpy().transpose((1, 2, 0))\n    plt.imshow(inp)\n    \n \n    if title is not None:\n        plt.title(title)\n    name  = 'showcase' + str(index) + '.png'\n    plt.savefig(name, bbox_inches='tight')\n    \n    plt.pause(0.001)  # pause a bit so that plots are updated","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\ndef bleu_score_checker():\n    gc = []\n    test = []\n    # Will execute only 200 times. Inner 5 times\n    offset = 153915\n    \n    for i in range(0,5000,5):\n        temp_gc = []\n        encoder.eval()\n        decoder.eval()\n        with torch.no_grad():\n            img, caption = dataset.evaluation(dataset,i + offset)\n            img = img.unsqueeze(0)\n#             print(caption)\n            \n            encoded_output = encoder(img.to(device))\n            \n            caps = decoder.beam_search(encoded_output, 3)\n            \n            caps = [dataset.vocab.itos[idx] for idx in caps]\n            \n            generated_caption = ' '.join(caps)\n#             show_image2(img.squeeze(0),i,title=generated_caption)\n            generated_caption = generated_caption.split()[1:]\n            generated_caption = generated_caption[:-2]\n            test.append(generated_caption)\n            temp_gc.append(caption)\n        for j in range(1,5):\n            img, caption = dataset.evaluation(dataset, i + j + offset)\n            temp_gc.append(caption)\n        gc.append(temp_gc)\n        decoder.train()\n        encoder.train()\n    print(\"-\" * 80)\n    print(\"Torch metrics\")\n    print(\"BLEU-1\", bleu_score(test,gc, max_n = 1, weights = [1.0] ) )\n    \n    print(\"BLEU-2\", bleu_score(test, gc, max_n = 2, weights = [0.5,0.5]))\n    \n    print(\"BLEU-3\", bleu_score(test, gc, max_n = 3, weights = [1/3,1/3,1/3]))\n    \n    print(\"BLEU-4\", bleu_score(test, gc, max_n = 4, weights = [0.25,0.25, 0.25, 0.25]))\n    \n    \n    print(\"-\"*80)\n    print(\"Nltk metrics\")\n    BLEU4 = nltk.translate.bleu_score.corpus_bleu(gc, test,weights=(0.25,0.25,0.25,0.25))\n    BLEU1 = nltk.translate.bleu_score.corpus_bleu(gc, test,weights=(1.0,0,0,0))\n    BLEU2 = nltk.translate.bleu_score.corpus_bleu(gc, test,weights=(0.5,0.5,0,0))\n    BLEU3 = nltk.translate.bleu_score.corpus_bleu(gc, test,weights=(0.33,0.33,0.33,0))\n    \n    \n    print(f\"BLEU-1 {BLEU1}\")\n    print(f\"BLEU-2 {BLEU2}\")\n    print(f\"BLEU-3 {BLEU3}\")\n    print(f\"BLEU-4 {BLEU4}\")\n    \n#     print(\"GC\" , gc)\n#     print(\"Predictions\", test)\n        \n    ","execution_count":48,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bleu_score_checker()","execution_count":50,"outputs":[{"output_type":"stream","text":"--------------------------------------------------------------------------------\nTorch metrics\nBLEU-1 0.5904854430327624\nBLEU-2 0.40363062428799307\nBLEU-3 0.2730071269421867\nBLEU-4 0.1869351372648633\n--------------------------------------------------------------------------------\nNltk metrics\nBLEU-1 0.6076569796548017\nBLEU-2 0.4153683443070568\nBLEU-3 0.28448294552872605\nBLEU-4 0.19237128974019688\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Epoch 10 - It's good over 5000/5 images\n\nTorch metrics\nBLEU-1 0.5921179879859123\nBLEU-2 0.40628215260341893\nBLEU-3 0.27486911607441983\nBLEU-4 0.18863532216414042\nNltk metrics\nBLEU-1 0.6074745138570442\nBLEU-2 0.41681903972525775\nBLEU-3 0.28552963821582755\nBLEU-4 0.19352754860699"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}